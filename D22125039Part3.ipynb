{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import joblib\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn import svm\n",
    "from tensorflow.keras.layers import Input,Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM, SimpleRNN, Reshape\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Concatenate\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 -  BBC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip file from a URL\n",
    "url = 'https://storage.googleapis.com/kaggle-data-sets/30569/38997/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230508%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230508T172143Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4e609a86d435ed376361a76cdd011b242c015e86fc2d0a8f415fecbb53a0ee3ef71e4f0b12222c0f4952572db08e96f6588a430a1acae8924a9a484181337f2013e3a68dfc73e0e1f96ad153c62d25b36d40b33992033027fde63c06e2b1c6291fc699b1817c83fd0c5ca52745b15247b5389b6030db1a51c9183cf5cdd4e41aeb78182b7ff83ff06a494c03ba09243b87a1aeda606bb1928e772f6c420ea6c7d1fdc54714ef2b07cf849fa533c9ef797584d44a05d036877966c99a35a48052e7a46877b17943f692a74fb3e82bd7c20fcef9ccdda38481291835c58668beaf481776dca8b1bc2921bef4567b11b4618213bff468e8884647638b48accb7f7d'\n",
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "\n",
    "# Extract the files to a directory\n",
    "z.extractall('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv('datasets/bbc-text.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert the text to a string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove unwanted characters using regular expressions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the words back into a single string\n",
    "    preprocessed_text = ' '.join(words)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "# Preprocess the text extract in the extractframe\n",
    "dataset['text'] = dataset['text'].apply(preprocess_text)\n",
    "dataset['category'] = dataset['category'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset to include only 'business' and 'sport' categories\n",
    "dataset = dataset[dataset['category'].isin(['business', 'sport'])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Fixed sequence lengths for Inputs and One-Hot Encode target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum number of words in a sequence\n",
    "maxlen = 20\n",
    "\n",
    "# Tokenize input sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "word_index = tokenizer.word_index\n",
    "max_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad sequences to have a fixed length\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Create one-hot encoded labels\n",
    "labels = pd.get_dummies(dataset['category']).values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training, testing, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes of the training, testing, and validation sets\n",
    "test_sample = int(0.1 * data.shape[0])\n",
    "validation_sample = int(0.1 * data.shape[0])\n",
    "train_sample = data.shape[0] - test_sample - validation_sample\n",
    "\n",
    "# Split the data and labels into training, testing, and validation sets\n",
    "x_train = data[:train_sample]\n",
    "y_train = labels[:train_sample]\n",
    "x_test = data[train_sample:train_sample+test_sample]\n",
    "y_test = labels[train_sample:train_sample+test_sample]\n",
    "x_val = data[train_sample+test_sample:]\n",
    "y_val = labels[train_sample+test_sample:]\n",
    "\n",
    "#Define hyperparamters\n",
    "embedding_dim = 16\n",
    "epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with multiple layers for creating the generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modelGenerate architecture\n",
    "modelGenerate = Sequential()\n",
    "modelGenerate.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "modelGenerate.add(LSTM(embedding_dim, return_sequences=True))\n",
    "modelGenerate.add(Dropout(0.2))\n",
    "modelGenerate.add(LSTM(embedding_dim))\n",
    "modelGenerate.add(Dropout(0.2))\n",
    "modelGenerate.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile modelGenerate\n",
    "modelGenerate.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = modelGenerate.fit(x_train, y_train, epochs= epochs, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGenerate.save(\"/Users/tobi/SavedModels/modelGenerate.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Generative Model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the seed text\n",
    "start_index = np.random.randint(len(dataset[dataset['category'] == 'sport']) - 1)\n",
    "seed_text = dataset[dataset['category'] == category].iloc[start_index]['text']\n",
    "\n",
    "# Tokenize the seed text\n",
    "seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "# Pad the sequence to a fixed length\n",
    "seed_sequence = pad_sequences([seed_sequence], maxlen=20, padding='pre', truncating='pre')\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_text(model, tokenizer, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict(seed_sequence, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word_index = np.argmax(yhat)\n",
    "        out_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == out_word_index:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += \" \" + out_word\n",
    "\n",
    "    print(in_text)\n",
    "\n",
    "#generate new text\n",
    "generated = generate_text(modelGenerate,tokenizer, seed_text, 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "loss, accuracy = modelGenerate.evaluate(x_test, y_test)\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(loss)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Loss: {:.4f}\".format(loss))\n",
    "print(\"Test Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Perplexity: {:.4f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss over time\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy over time\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
