{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn import svm\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input,Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM, SimpleRNN, Reshape\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Concatenate\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 -  BBC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip file from a URL\n",
    "url = 'https://storage.googleapis.com/kaggle-data-sets/30569/38997/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230508%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230508T172143Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4e609a86d435ed376361a76cdd011b242c015e86fc2d0a8f415fecbb53a0ee3ef71e4f0b12222c0f4952572db08e96f6588a430a1acae8924a9a484181337f2013e3a68dfc73e0e1f96ad153c62d25b36d40b33992033027fde63c06e2b1c6291fc699b1817c83fd0c5ca52745b15247b5389b6030db1a51c9183cf5cdd4e41aeb78182b7ff83ff06a494c03ba09243b87a1aeda606bb1928e772f6c420ea6c7d1fdc54714ef2b07cf849fa533c9ef797584d44a05d036877966c99a35a48052e7a46877b17943f692a74fb3e82bd7c20fcef9ccdda38481291835c58668beaf481776dca8b1bc2921bef4567b11b4618213bff468e8884647638b48accb7f7d'\n",
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "\n",
    "# Extract the files to a directory\n",
    "z.extractall('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a variable called \"data\"\n",
    "data = pd.read_csv('datasets/bbc-text.csv')\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip file from a URL\n",
    "url = 'https://storage.googleapis.com/kaggle-data-sets/3240578/5636667/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230508%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230508T205651Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=7fb26ad2f78ece1c96f8155b1488554f60ff075d92c41361a28343cdb228d7e19ec31d512251967cde6488323aebbf4bb8b8a791dc46f33d2cc42c6cfb399465174c27a74d3719683d9643acda27157f9aa10aacd14170889eba102e4aa05237f41201934a10944007068264e9174611d0f9d9f5dd47cb4a1e02280e3f2b1c25b4be7746aee228ba0ee5de09bc0868dce80fa8b295cc637fa24e4906a87a904b4e1300e723467593727012201fd3625a30a8837418a57c089e56c128017247dac8c71bd0187da3a700988210d6d659538ac3ffef20dccd3bb5a270479056f32910c2ee319a3eaf4f9ce2bdb2be3f08d17ab0a146d863b1653db32aebee0b2786'\n",
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "\n",
    "# Extract the files to a directory\n",
    "z.extractall('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['modelLSTM2', 'modelRNN', 'modelTwoInputs', 'modelTransfserLSTM2', 'modelGenerate', 'modelAltCNNScratch', 'modelAltCNN', 'modelBagOfWords', 'modelLSTM', 'modelOnTheFly', 'modelTransfserLSTM', 'modelLSTMCNN']\n"
     ]
    }
   ],
   "source": [
    "model_names = []\n",
    "for file_path in glob.glob(\"models/*.keras\"):\n",
    "    model_names.append(file_path.split(\"/\")[-1].split(\".keras\")[0])\n",
    "\n",
    "print(model_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert the text to a string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove unwanted characters using regular expressions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the words back into a single string\n",
    "    preprocessed_text = ' '.join(words)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "# Preprocess the text extract in the extractframe\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "data['category'] = data['category'].apply(preprocess_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Truncated dataset into Training, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing text and Pad sequences to ensure equal length\n",
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map the labels to numerical values\n",
    "label_dict = {label: index for index, label in enumerate(data['category'].unique())}\n",
    "y = [label_dict[label] for label in data['category']]\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters that are used to define the architecture and settings of the RNN model\n",
    "max_features = 5000  #maximum number of words to keep based on word frequency\n",
    "maxlen = 400 #maximum number of words in a single sentence.\n",
    "embedding_dims = 16 #dimensionality of the output space\n",
    "epochs = 5 #iterations\n",
    "\n",
    "# Preprocess by padding the sequences to the same length \n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=maxlen)\n",
    "\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape:', X_test.shape)\n",
    "print('Validation shape:', X_val.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Variant from Scratch (CNN model) - No transfer Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model architecture\n",
    "modelAltCNNScratch = Sequential()\n",
    "modelAltCNNScratch.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "modelAltCNNScratch.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "modelAltCNNScratch.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "modelAltCNNScratch.add(Dropout(0.2))\n",
    "modelAltCNNScratch.add(GlobalMaxPooling1D())\n",
    "modelAltCNNScratch.add(Dense(len(label_dict), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "modelAltCNNScratch.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data\n",
    "historymodelAltCNNScratch = modelAltCNNScratch.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelAltCNNScratch.save(\"/Users/tobi/SavedModels/modelAltCNNScratch.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = modelAltCNNScratch.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test Loss:\" , loss)\n",
    "print(\"Test Accuracy:\" , accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "plt.plot(historymodelAltCNNScratch.history['accuracy'])\n",
    "plt.plot(historymodelAltCNNScratch.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(historymodelAltCNNScratch.history['loss'])\n",
    "plt.plot(historymodelAltCNNScratch.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning (I) - New CNN model with input from previous CNN model created in Part 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_26 (Embedding)    (None, 400, 16)           80000     \n",
      "                                                                 \n",
      " conv1d_29 (Conv1D)          (None, 398, 64)           3136      \n",
      "                                                                 \n",
      " conv1d_30 (Conv1D)          (None, 395, 64)           16448     \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 395, 64)           0         \n",
      "                                                                 \n",
      " global_max_pooling1d_14 (Gl  (None, 64)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 87)                5655      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,239\n",
      "Trainable params: 105,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_cnn_alt_model = load_model('models/modelAltCNN.keras')\n",
    "pretrained_cnn_alt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_maxlen = pretrained_cnn_alt_model.input_shape[1]\n",
    "print(transfer_maxlen)\n",
    "num_filters_layer1 = pretrained_cnn_alt_model.layers[1].filters\n",
    "num_filters_layer2 = pretrained_cnn_alt_model.layers[2].filters\n",
    "num_filters_layer1\n",
    "num_filters_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model architecture\n",
    "modelTransfserLSTM = Sequential()\n",
    "modelTransfserLSTM.add(pretrained_cnn_alt_model)\n",
    "modelTransfserLSTM.add(Reshape((1, -1)))\n",
    "modelTransfserLSTM.add(LSTM(embedding_dims, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelTransfserLSTM.add(Dense(len(label_dict), activation='softmax'))\n",
    "\n",
    "# Freeze the weights of the pretrained CNN layers\n",
    "for layer in modelTransfserLSTM.layers[0].layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "modelTransfserLSTM.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data\n",
    "historymodelTransfserLSTM = modelTransfserLSTM.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTransfserLSTM.save(\"/Users/tobi/SavedModels/modelTransfserLSTM.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = modelTransfserLSTM.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test Loss:\" , loss)\n",
    "print(\"Test Accuracy:\" , accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "plt.plot(historytransferModel1.history['accuracy'])\n",
    "plt.plot(historytransferModel1.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(historytransferModel1.history['loss'])\n",
    "plt.plot(historytransferModel1.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning (II) - New CNN model with input from another CNN model created in Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_27 (Embedding)    (None, 400, 16)           80000     \n",
      "                                                                 \n",
      " conv1d_31 (Conv1D)          (None, 397, 64)           4160      \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 397, 64)           0         \n",
      "                                                                 \n",
      " global_max_pooling1d_15 (Gl  (None, 64)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " reshape_12 (Reshape)        (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_23 (LSTM)              (None, 16)                5184      \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 87)                1479      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,823\n",
      "Trainable params: 90,823\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_modelLSTMCNN_model = load_model('models/modelLSTMCNN.keras')\n",
    "pretrained_modelLSTMCNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model architecture\n",
    "modelTransferLSTM2 = Sequential()\n",
    "modelTransferLSTM2.add(pretrained_modelLSTMCNN_model)\n",
    "modelTransferLSTM2.add(Reshape((1, -1)))\n",
    "modelTransferLSTM2.add(LSTM(embedding_dims, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelTransferLSTM2.add(Dense(len(label_dict), activation='softmax'))\n",
    "\n",
    "# Freeze the weights of the pretrained CNN layers\n",
    "for layer in modelTransferLSTM2.layers[0].layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "modelTransferLSTM2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data\n",
    "historymodelTransferLSTM2 = modelTransferLSTM2.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTransferLSTM2.save(\"/Users/tobi/SavedModels/modelTransfserLSTM2.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = modelTransferLSTM2.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test Loss:\" , loss)\n",
    "print(\"Test Accuracy:\" , accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "plt.plot(historymodelTransferLSTM2.history['accuracy'])\n",
    "plt.plot(historymodelTransferLSTM2.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(historymodelTransferLSTM2.history['loss'])\n",
    "plt.plot(historymodelTransferLSTM2.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparisons "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Model Variant with no transfer learning to New CNN model with transfer learning I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historymodelAltCNNScratch.history['accuracy'], linestyle='solid', color='blue')\n",
    "plt.plot(historymodelAltCNNScratch.history['val_accuracy'], linestyle='dotted', color='blue')\n",
    "plt.plot(historymodelTransfserLSTM.history['accuracy'], linestyle='solid', color='orange')\n",
    "plt.plot(historymodelTransfserLSTM.history['val_accuracy'], linestyle='dotted', color='orange')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Model Variant with no transfer learning Train', 'Model Variant with no transfer learning Val', 'New CNN model with transfer learning I Train', 'New CNN model with transfer learning IVal'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Model Variant with no transfer learning to New CNN model with transfer learning II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historymodelAltCNNScratch.history['accuracy'], linestyle='solid', color='blue')\n",
    "plt.plot(historymodelAltCNNScratch.history['val_accuracy'], linestyle='dotted', color='blue')\n",
    "plt.plot(historymodelTransferLSTM2.history['accuracy'], linestyle='solid', color='orange')\n",
    "plt.plot(historymodelTransferLSTM2.history['val_accuracy'], linestyle='dotted', color='orange')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Model Variant with no transfer learning Train', 'Model Variant with no transfer learning Val', 'New CNN model with transfer learning II Train', 'New CNN model with transfer learning II Val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Transfer Learning Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historymodelTransfserLSTM.history['accuracy'], linestyle='solid', color='blue')\n",
    "plt.plot(historymodelTransfserLSTM.history['val_accuracy'], linestyle='dotted', color='blue')\n",
    "plt.plot(historymodelTransferLSTM2.history['accuracy'], linestyle='solid', color='orange')\n",
    "plt.plot(historymodelTransferLSTM2.history['val_accuracy'], linestyle='dotted', color='orange')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Model Variant with no transfer learning Train', 'Model Variant with no transfer learning Val', 'New CNN model with transfer learning I Train', 'New CNN model with transfer learning IVal'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
